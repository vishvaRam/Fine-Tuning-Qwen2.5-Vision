{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXElGny76giP"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers datasets accelerate peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration,Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import requests\n",
        "from torch.utils.data import DataLoader\n",
        "from io import BytesIO\n",
        "from torch.utils.data import Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "zcpdLukmg321"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the pre-trained model in 8-bit quantized mode\n",
        "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Use float16 for better performance\n",
        "    device_map=\"auto\",  # Automatically map layers to the GPU\n",
        "    load_in_8bit=True,  # Enable 8-bit quantization\n",
        ")\n",
        "\n",
        "# Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Load the processor\n",
        "processor = AutoProcessor.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ntxWCUkWhwOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank matrices\n",
        "    lora_alpha=32,  # Scaling factor for LoRA updates\n",
        "    target_modules=[\n",
        "        \"q_proj\",  # Query projection\n",
        "        \"k_proj\",  # Key projection\n",
        "        \"v_proj\",  # Value projection\n",
        "    ],\n",
        "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Do not train biases\n",
        "    task_type=\"CAUSAL_LM\",  # Task type: Causal Language Modeling\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "G1QS4BCChwb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "y4kxGwH-hwgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter and subset the dataset by split\n",
        "def filter_and_subset(single_dataset, split_name, subset_size=1000):\n",
        "    # Filter by split\n",
        "    filtered_dataset = single_dataset.filter(lambda example: example[\"split\"] == split_name)\n",
        "\n",
        "    # Select the first subset_size samples\n",
        "    return  filtered_dataset.shuffle(seed=42)\n",
        "\n",
        "# Subset the dataset for train, validation, and test splits\n",
        "train_dataset_subset = filter_and_subset(dataset, \"train\", subset_size=1000)\n",
        "val_dataset_subset = filter_and_subset(dataset, \"val\", subset_size=1000)\n",
        "test_dataset_subset = filter_and_subset(dataset, \"test\", subset_size=1000)"
      ],
      "metadata": {
        "id": "8N2d-4sbhwju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset_subset)\n",
        "print(val_dataset_subset)\n",
        "print(test_dataset_subset)"
      ],
      "metadata": {
        "id": "wCTvJ4c-hwmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "dataset_large = train_dataset_subset[\"test\"]  # 29000 samples\n",
        "dataset_medium = val_dataset_subset[\"test\"]  # 1014 samples\n",
        "dataset_small = test_dataset_subset[\"test\"]  # 1000 samples\n",
        "\n",
        "random_indices_large = random.sample(range(len(dataset_large)), 1000)\n",
        "random_indices_medium = random.sample(range(len(dataset_medium)), 150)\n",
        "random_indices_small = random.sample(range(len(dataset_small)), 150)\n",
        "\n",
        "train = dataset_large.select(random_indices_large)\n",
        "test = dataset_medium.select(random_indices_medium)\n",
        "val = dataset_small.select(random_indices_small)"
      ],
      "metadata": {
        "id": "4jAKU2_tsb9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train\n",
        "test\n",
        "val"
      ],
      "metadata": {
        "id": "82Q9--m2v2sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "82CsN9Lb41Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del dataset\n",
        "del train_dataset_subset\n",
        "del val_dataset_subset\n",
        "del test_dataset_subset"
      ],
      "metadata": {
        "id": "8DbHNnfVwVDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocess Data - Randomly sample one caption per image\n",
        "def preprocess_data(example):\n",
        "    caption = random.choice(example[\"caption\"])  # Randomly sample one caption\n",
        "    return {\"image\": example[\"image\"], \"caption\": caption}\n",
        "\n",
        "# Apply preprocessing to all subsets\n",
        "train_dataset_processed = train.map(preprocess_data, batched=False)\n",
        "val_dataset_processed = test.map(preprocess_data, batched=False)\n",
        "test_dataset_processed = val.map(preprocess_data, batched=False)\n"
      ],
      "metadata": {
        "id": "XZL3jiPe3LJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to all subsets\n",
        "print(\"Processing train dataset...\")\n",
        "train_dataset_processed = train.map(preprocess_data, batched=False, remove_columns=train.column_names)\n",
        "print(\"Processing validation dataset...\")\n",
        "val_dataset_processed = test.map(preprocess_data, batched=False, remove_columns=test.column_names)\n",
        "print(\"Processing test dataset...\")\n",
        "test_dataset_processed = val.map(preprocess_data, batched=False, remove_columns=val.column_names)"
      ],
      "metadata": {
        "id": "vXafT5xF5x7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the dataset to understand what \"image\" contains\n",
        "sample = train_dataset_processed[0]\n",
        "print(sample)\n",
        "print(f\"Sample image type: {type(sample['image'])}\")\n",
        "print(f\"Sample caption: {sample['caption']}\")"
      ],
      "metadata": {
        "id": "k2d2Kvcw3LHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data collator function with proper image handling\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    captions = []\n",
        "\n",
        "    for example in batch:\n",
        "        image_path_or_obj = example[\"image\"]\n",
        "        caption = example[\"caption\"]\n",
        "\n",
        "        # Load image - handle different possible types\n",
        "        try:\n",
        "            # If already a PIL Image\n",
        "            if isinstance(image_path_or_obj, Image.Image):\n",
        "                image = image_path_or_obj.convert(\"RGB\")\n",
        "            # If it's a string path or URL\n",
        "            elif isinstance(image_path_or_obj, str):\n",
        "                if image_path_or_obj.startswith(\"http\"):\n",
        "                    response = requests.get(image_path_or_obj)\n",
        "                    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                else:\n",
        "                    image = Image.open(image_path_or_obj).convert(\"RGB\")\n",
        "            else:\n",
        "                raise TypeError(f\"Unsupported image type: {type(image_path_or_obj)}\")\n",
        "\n",
        "            images.append(image)\n",
        "            captions.append(caption)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image: {e}\")\n",
        "            continue  # Skip this example if there's an error\n",
        "\n",
        "    if not images:  # Handle case where all images failed to load\n",
        "        raise ValueError(\"No valid images found in batch\")\n",
        "\n",
        "    # Process inputs\n",
        "    inputs = processor(\n",
        "        text=captions,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "    # Labels are the same as input_ids for language modeling\n",
        "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "rgE-pV2O3LDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = collate_fn([train_dataset_processed[0]])\n",
        "print(sample.keys())\n",
        "print(sample[\"input_ids\"].shape)\n",
        "print(sample[\"attention_mask\"].shape)\n",
        "print(sample[\"labels\"].shape)"
      ],
      "metadata": {
        "id": "PCCe6Aty65PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,  # Small batch size to fit in memory\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,  # Simulate larger batch sizes\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    save_total_limit=2,  # Save only the best two checkpoints\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "SgvG31UZ3K_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the trainer with the data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_processed,\n",
        "    eval_dataset=val_dataset_processed,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "PLp5XSAP3K82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tune the Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "2vk1VAQl3K60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the Model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "id": "kuwjskyu3K3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "os.makedirs(\"./fine_tuned_qwen2.5_vl_lora\", exist_ok=True)\n",
        "model.save_pretrained(\"./fine_tuned_qwen2.5_vl_lora\")\n",
        "processor.save_pretrained(\"./fine_tuned_qwen2.5_vl_lora\")"
      ],
      "metadata": {
        "id": "CG4vzwfi3K0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}