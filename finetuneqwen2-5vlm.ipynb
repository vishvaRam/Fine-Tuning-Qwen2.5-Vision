{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install tensorboard","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip uninstall -y transformers","metadata":{"_uuid":"30a7214d-3179-498f-be35-01647919cec1","_cell_guid":"998b94f4-25b3-4ee9-aee4-d7a9c05ba231","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils accelerate","metadata":{"_uuid":"2b5ff170-9c4a-42c5-9c05-5254c6948333","_cell_guid":"831a4a35-f471-475c-a617-03a4f9cdb130","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show transformers","metadata":{"_uuid":"663d13d3-b684-42b9-bec4-4ef6f0677268","_cell_guid":"49673c5b-1cb7-409a-8fd0-4d5bbd48a8b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" !pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121","metadata":{"_uuid":"606849d7-10f8-4954-8215-c757818d2630","_cell_guid":"36d03379-811d-4803-a522-3600dfbe53e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"","metadata":{"_uuid":"1857cfe4-387e-4308-b169-ec7543563bb9","_cell_guid":"0e637134-2df5-46f7-bcd3-2938f33bf683","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_data(sample):\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": sample[\"image\"],\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": sample[\"query\"],\n                },\n            ],\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n        },\n    ]","metadata":{"_uuid":"f8a0e8ad-143f-4f17-a2ff-3ff7bfdab67e","_cell_guid":"93e96f17-33e9-4412-bfda-5f846a03767c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_id = \"HuggingFaceM4/ChartQA\"\ntrain_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=[\"train[:1%]\", \"val[:1%]\", \"test[:1%]\"])","metadata":{"_uuid":"f6e6dd74-862f-4fc4-a76e-c16d91c4e797","_cell_guid":"53851367-15b6-49f1-be96-db6b88aae189","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset\n\n# Dataset({\n#     features: ['image', 'query', 'label', 'human_or_machine'],\n#     num_rows: 283\n# })","metadata":{"_uuid":"72dc5ac5-7a41-495d-b8b1-eaac8f71ee8d","_cell_guid":"8ed17050-6241-46f0-8a84-e68819f65cb3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = [format_data(sample) for sample in train_dataset]\neval_dataset = [format_data(sample) for sample in eval_dataset]\ntest_dataset = [format_data(sample) for sample in test_dataset]","metadata":{"_uuid":"04045762-aac8-4e6c-913e-b0d4bcbb6332","_cell_guid":"b9c7e918-fa5a-4f4c-895b-cce75ca19c03","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[10]\n\n# [{'role': 'system',\n#   'content': [{'type': 'text',\n#     'text': 'You are a Vision Language Model specialized in interpreting visual data from chart images.\\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n#  {'role': 'user',\n#   'content': [{'type': 'image',\n#     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=307x429>},\n#    {'type': 'text', 'text': \"What's the rightmost value dark brown graph?\"}]},\n#  {'role': 'assistant', 'content': [{'type': 'text', 'text': '47'}]}]","metadata":{"_uuid":"a67950b2-9c08-4497-8c7a-9b4e01214df4","_cell_guid":"d7fa19a8-554d-4fa9-895b-d40292c89757","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n\nmodel_id= \"Qwen/Qwen2.5-VL-3B-Instruct\"","metadata":{"_uuid":"9c1f64dd-1a6f-427d-bc76-d0909d8cafd5","_cell_guid":"c2682548-215d-4071-8d75-c88b2e9cbb8c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"_uuid":"13ae21f8-985f-4eed-a30b-a6100612205c","_cell_guid":"6dde700a-c8b4-4775-b5d2-1292ea446ad6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0]\n\n# [{'role': 'system',\n#   'content': [{'type': 'text',\n#     'text': 'You are a Vision Language Model specialized in interpreting visual data from chart images.\\nYour task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\\nThe charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\\nFocus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.'}]},\n#  {'role': 'user',\n#   'content': [{'type': 'image',\n#     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=422x359>},\n#    {'type': 'text', 'text': 'Is the value of Favorable 38 in 2015?'}]},\n#  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Yes'}]}]","metadata":{"_uuid":"626cd51b-83eb-4eb9-b6d7-8a55beac3d4f","_cell_guid":"350ab8ec-abcf-4650-a70b-6b6aa5d3e8bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][1:2]\n\n# [{'role': 'user',\n#   'content': [{'type': 'image',\n#     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=422x359>},\n#    {'type': 'text', 'text': 'Is the value of Favorable 38 in 2015?'}]}]","metadata":{"_uuid":"e9fa9ab0-4a1b-4d3e-a8c0-fe4c07247d83","_cell_guid":"7f1d60f2-1a3f-4406-8b0f-490f26a9242b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][1][\"content\"][0][\"image\"]","metadata":{"_uuid":"95cc0efb-93bd-4e68-b7ab-1231c1dd1b93","_cell_guid":"679c4170-0b71-45ba-9a45-7f3baee187fe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from qwen_vl_utils import process_vision_info\n\n# Set device dynamically based on GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\ndef generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=device):\n    # Prepare the text input by applying the chat template\n    text_input = processor.apply_chat_template(\n        sample[1:2], tokenize=False, add_generation_prompt=True  # Use the sample without the system message\n    )\n\n    # Process the visual input from the sample\n    image_inputs, _ = process_vision_info(sample)\n\n    # Prepare the inputs for the model\n    model_inputs = processor(\n        text=[text_input],\n        images=image_inputs,\n        return_tensors=\"pt\",\n    ).to(\n        device\n    )  # Move inputs to the specified device\n\n    # Generate text with the model\n    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n\n    # Trim the generated ids to remove the input ids\n    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n\n    # Decode the output text\n    output_text = processor.batch_decode(\n        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n\n    return output_text[0]  # Return the first decoded output text","metadata":{"_uuid":"e0e802ab-54b7-4765-b760-25a6b9fb1095","_cell_guid":"62745045-d939-4274-a88e-71a0ec742f00","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of how to call the method with sample:\noutput = generate_text_from_sample(model, processor, train_dataset[0])\noutput\n\n# 'Yes, the value of \"Favorable\" is 38 in 2015 according to the provided data.'","metadata":{"_uuid":"4a9a91d2-c1d5-41c7-a63d-1339d44d51de","_cell_guid":"fd15a765-1b28-4485-a939-abe747612a03","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport time\n\n\ndef clear_memory():\n    if \"inputs\" in globals():\n        del globals()[\"inputs\"]\n    if \"model\" in globals():\n        del globals()[\"model\"]\n    if \"processor\" in globals():\n        del globals()[\"processor\"]\n    if \"trainer\" in globals():\n        del globals()[\"trainer\"]\n    if \"peft_model\" in globals():\n        del globals()[\"peft_model\"]\n    if \"bnb_config\" in globals():\n        del globals()[\"bnb_config\"]\n    time.sleep(2)\n\n    # Garbage collection and clearing CUDA memory\n    gc.collect()\n    time.sleep(2)\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    time.sleep(2)\n    gc.collect()\n    time.sleep(2)\n\n    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n\n\nclear_memory()","metadata":{"_uuid":"e958e374-6c8a-4fe6-9e76-364636af490e","_cell_guid":"2d4b22cb-883c-4016-a2f4-965523b5fd1b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\n\nif device == \"cuda\":\n    # BitsAndBytesConfig int-4 config for GPU\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, \n        bnb_4bit_use_double_quant=True, \n        bnb_4bit_quant_type=\"nf4\", \n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\n    # Load model with quantization config for GPU\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        model_id, \n        device_map=\"auto\",\n        quantization_config=bnb_config,\n        use_cache= True\n    )\n    processor = AutoProcessor.from_pretrained(model_id)\nelse:\n    # Load model without quantization config for CPU\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        model_id, \n        torch_dtype=torch.bfloat16,\n        use_cache= True\n    )\n    processor = AutoProcessor.from_pretrained(model_id)","metadata":{"_uuid":"83a7a3a0-b608-4258-af89-88c3f713213d","_cell_guid":"8953f545-ed51-43cc-a021-265cb681ff0b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tok = AutoTokenizer.from_pretrained(model_id)\ntok.padding_side = \"right\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# Configure LoRA\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.05,\n    r=8,\n    bias=\"none\",\n    target_modules=[\n        \"q_proj\", \n        \"v_proj\"\n    ],\n    task_type=\"CAUSAL_LM\",\n)\n\npeft_model = get_peft_model(model, peft_config)\n\npeft_model.print_trainable_parameters()\n\n# trainable params: 1,843,200 || all params: 3,756,466,176 || trainable%: 0.0491","metadata":{"_uuid":"260af49d-b9d8-4763-bd80-9ab6638235b8","_cell_guid":"5e476ebc-8f43-46a8-a4d1-f10f84d956a1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTConfig\n\n# Configure training arguments\ntraining_args = SFTConfig(\n    output_dir=\"qwen2.5-3b-instruct-trl-sft-ChartQA\",  # Directory to save the model\n    num_train_epochs=1,  # Number of training epochs\n    per_device_train_batch_size=2,  # Batch size for training\n    per_device_eval_batch_size=2,  # Batch size for evaluation\n    gradient_accumulation_steps=4,  # Steps to accumulate gradients\n    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n    # Optimizer and scheduler settings\n    optim=\"adamw_torch_fused\",  # Optimizer type\n    learning_rate=2e-4,  # Learning rate for training\n    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n    # Logging and evaluation\n    logging_steps=10,  # Steps interval for logging\n    eval_steps=10,  # Steps interval for evaluation\n    eval_strategy=\"steps\",  # Strategy for evaluation\n    save_strategy=\"steps\",  # Strategy for saving the model\n    save_steps=10,  # Steps interval for saving\n    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n    greater_is_better=False,  # Whether higher metric values are better\n    load_best_model_at_end=True,  # Load the best model after training\n    # Mixed precision and gradient settings\n    bf16=True,  # Use bfloat16 precision\n    tf32=False,  # Use TensorFloat-32 precision\n    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n    warmup_ratio=0,  # Ratio of total steps for warmup\n    # Hub and reporting\n    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n    report_to=\"tensorboard\",  # Reporting tool for tracking metrics\n    # Gradient checkpointing settings\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n    # Dataset configuration\n    dataset_text_field=\"\",  # Text field in dataset\n    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n    # max_seq_length=1024  # Maximum sequence length for input\n)\n\ntraining_args.remove_unused_columns = False  # Keep unused columns in dataset","metadata":{"_uuid":"d280e8c1-9ca2-45cf-97f5-a2364a33280e","_cell_guid":"ecf74796-c72a-4e71-b68e-5125a0cf3b38","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%tensorboard --logdir qwen2.5-3b-instruct-trl-sft-ChartQA","metadata":{"_uuid":"107a4534-1127-43d4-89dd-d6ec4e7c7be8","_cell_guid":"5d7b0566-5381-4854-8990-941764e2e0eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a data collator to encode text and image pairs\ndef collate_fn(examples):\n    # Get the texts and images, and apply the chat template\n    texts = [\n        processor.apply_chat_template(example, tokenize=False) for example in examples\n    ]  # Prepare texts for processing\n    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n\n    # Tokenize the texts and process the images\n    batch = processor(\n        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n    )  # Encode texts and images into tensors\n\n    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n\n    # Ignore the image token index in the loss computation (model specific)\n    if isinstance(processor, AutoTokenizer):  # Check if the processor is Qwen2VLProcessor\n        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n    else:\n        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n\n    # Mask image token IDs in the labels\n    for image_token_id in image_tokens:\n        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n\n    batch[\"labels\"] = labels  # Add labels to the batch\n\n    return batch  # Return the prepared batch","metadata":{"_uuid":"a636a115-c5d0-471f-88b6-f221eac6d39a","_cell_guid":"c622b55f-e080-4e83-a31f-06fa3a513b1a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collate_fn,\n    peft_config=peft_config,\n    processing_class=processor\n    # tokenizer=tok,\n)","metadata":{"_uuid":"2554d8ca-7150-4d59-86cd-60bb4c1c7b4d","_cell_guid":"ade7985b-e7f0-4a1a-b1ad-3d83c4734fac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\n\n# TrainOutput(global_step=35, training_loss=2.625527776990618, metrics={'train_runtime': 1476.0926, \n# 'train_samples_per_second': 0.192, 'train_steps_per_second': 0.024, 'total_flos': 2509185336508416.0, 'train_loss': 2.625527776990618})\n\n# Step\tTraining Loss\tValidation Loss\n# 10\t3.826300\t3.099709\n# 20\t2.762000\t2.329909\n# 30\t1.952900\t1.452009","metadata":{"_uuid":"b19a2898-76eb-4f45-a5f3-f83fcfc98645","_cell_guid":"279aea04-9a39-471a-85e8-d5722546b75c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(training_args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_memory()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"/kaggle/working/qwen2.5-3b-instruct-trl-sft-ChartQA\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\nprocessor = AutoProcessor.from_pretrained(model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = generate_text_from_sample(model, processor, train_dataset[0])\noutput\n\n# 'Yes, the value of \"Favorable\" is 38 in 2015 according to the provided data.'","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}